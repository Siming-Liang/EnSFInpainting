{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the following variables before each experiment\n",
    "# exptname          output file name\n",
    "# filename_climo    input file 64/256   3hr/12hr\n",
    "# filename_truth    input file\n",
    "# nobs              number of observation nobs/64*64 nobs/256*256\n",
    "# pickarctan        0 is fully linear, nobs is fully nonlinear\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import sys, time, os\n",
    "from sqgturb import SQG, rfft2, irfft2, cartdist,enkf_update,gaspcohn, bulk_ensrf\n",
    "\n",
    "from EnSF_Sparse_obs_dct import EnSF\n",
    "from skimage.restoration import inpaint\n",
    "import cv2\n",
    "from scipy.stats import ortho_group\n",
    "from sklearn import decomposition\n",
    "import cvxpy as cp\n",
    "#plot\n",
    "import scipy.fft\n",
    "# from scipy.fftpack import fft, dct, idct\n",
    "import torch\n",
    "import torch_dct as dct\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def l1_solve(components_, masked_img, mask, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve for vector v = (v1, ..., vN) with L1-regularization on v:\n",
    "        minimize ||A1*v1 + A2*v2 + ... + AN*vN - B||_F^2 + lambda * ||v||_1\n",
    "    \"\"\"\n",
    "    \n",
    "    A = components_.T\n",
    "\n",
    "    # Define the optimization variables\n",
    "    v = cp.Variable(np.shape(A)[1])\n",
    "\n",
    "    B = masked_img.ravel()\n",
    "    mask_flatten = mask.ravel()\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        cp.norm(cp.multiply(mask_flatten,(A @ v - B)), 2)**2 + lambda_reg * cp.norm(v, 1)\n",
    "    )\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    problem = cp.Problem(objective)\n",
    "    problem.solve()\n",
    "\n",
    "    return v.value\n",
    "\n",
    "def random_indices_from_flatten(shape, num_samples):\n",
    "    # Generate random flat indices\n",
    "    flat_indices = torch.randperm(shape * shape)[:num_samples]\n",
    "    \n",
    "    # Convert to 2D indices\n",
    "    rows = flat_indices // shape\n",
    "    cols = flat_indices % shape\n",
    "    return rows, cols\n",
    "\n",
    "def unused_indices(shape,data_rows,data_cols):\n",
    "\n",
    "    if len(data_rows) == shape **2:\n",
    "        remaining_rows = torch.tensor([0])\n",
    "        remaining_cols = torch.tensor([0])\n",
    "    else:\n",
    "        all_indices = torch.arange(shape * shape)  \n",
    "        selected_indices = torch.cat([data_rows * shape + data_cols])\n",
    "        selected_indices = data_rows * shape + data_cols\n",
    "        remaining_indices = torch.tensor(list(set(all_indices.tolist()) - set(selected_indices.tolist())))\n",
    "        remaining_rows = remaining_indices // shape\n",
    "        remaining_cols = remaining_indices % shape\n",
    "\n",
    "    return remaining_rows, remaining_cols\n",
    "    \n",
    "def k_largest_indices(tensor: torch.Tensor, k: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Find the indices of the k largest entries in a 2D tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input 2D tensor\n",
    "        k: Number of largest entries to find\n",
    "        \n",
    "    Returns:\n",
    "        tuple of (row indices, column indices) for the k largest entries\n",
    "    \"\"\"\n",
    "    # Flatten the tensor and get indices of k largest values\n",
    "    flat_indices = torch.topk(tensor.flatten(), k).indices\n",
    "    \n",
    "    # Convert flat indices back to 2D coordinates\n",
    "    num_cols = tensor.size(1)\n",
    "    row_indices = flat_indices // num_cols\n",
    "    col_indices = flat_indices % num_cols\n",
    "    #print(flat_indices)\n",
    "    return row_indices, col_indices\n",
    "\n",
    "def get_indices_with_sum_less_than_k(rows: int, cols: int, k: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate row and column indices where i + j < k for a rows Ã— cols array\n",
    "    \n",
    "    Args:\n",
    "        rows (int): Number of rows\n",
    "        cols (int): Number of columns\n",
    "        k (int): Threshold value for sum of indices\n",
    "        \n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Row indices and column indices\n",
    "    \"\"\"\n",
    "    # Create meshgrid of indices\n",
    "    row_indices = torch.arange(rows).repeat_interleave(cols)\n",
    "    col_indices = torch.arange(cols).repeat(rows)\n",
    "    \n",
    "    # Create mask where sum of indices is less than k\n",
    "    mask = row_indices + col_indices < k\n",
    "    \n",
    "    # Return row and column indices where condition is met\n",
    "    return row_indices[mask], col_indices[mask]\n",
    "\n",
    "def get_ensemble_covariance(num_ensemble,ensemble):\n",
    "    ensemble_mean = ensemble.mean(dim=0)\n",
    "    ensemble_perturbations = ensemble - ensemble_mean  \n",
    "    cov_matrix = (ensemble_perturbations.T @ ensemble_perturbations) / (num_ensemble - 1)\n",
    "    return cov_matrix\n",
    "\n",
    "def total_variation_loss(x, height=64, width=64):\n",
    "    # x is expected to be of shape [batch_size, height, width]\n",
    "    x = x.view(-1, 1, height, width)  # Reshape to [batch_size, 1, height, width]\n",
    "    \n",
    "    # Compute TV loss for each image in the batch\n",
    "    tv_h = torch.mean(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]), dim=[1, 2, 3])\n",
    "    tv_w = torch.mean(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]), dim=[1, 2, 3])\n",
    "    \n",
    "    # Return the total variation for each image\n",
    "    return tv_h + tv_w\n",
    "\n",
    "def inpaint_task(i):\n",
    "    return inpaint.inpaint_biharmonic(masked_img[i], 1.0 - img_mask)\n",
    "\n",
    "# Function for advancing models\n",
    "def advance_task(nanal):\n",
    "    return models[nanal].advance(pvens[nanal])\n",
    "\n",
    "def process_image(i):\n",
    "    return cv2.inpaint(cv2_raw_img[i], cv2_mask, 2, cv2.INPAINT_NS) / 255\n",
    "\n",
    "# horizontal covariance localization length scale in meters.\n",
    "hcovlocal_scale = 1 #float(sys.argv[1])\n",
    "\n",
    "covinflate1 = 1\n",
    "covinflate2 = -1\n",
    "exptname = os.getenv('exptname','Arctan_EnSF_12hrly_64_cv2_25per')   #  Sparse  Arctan\n",
    "threads = int(os.getenv('OMP_NUM_THREADS','1'))\n",
    "\n",
    "diff_efold = None # use diffusion from climo file\n",
    "\n",
    "profile = False # turn on profiling?\n",
    "\n",
    "use_letkf = True  # use LETKF\n",
    "global_enkf = False # global EnSRF solve\n",
    "read_restart = False\n",
    "# if savedata not None, netcdf filename will be defined by env var 'exptname'\n",
    "# if savedata = 'restart', only last time is saved (so expt can be restarted)\n",
    "#savedata = True \n",
    "#savedata = 'restart'\n",
    "savedata = None\n",
    "#nassim = 101 \n",
    "#nassim_spinup = 1\n",
    "nassim = 100 # assimilation times to run\n",
    "nassim_spinup = 100\n",
    "\n",
    "direct_insertion = False \n",
    "if direct_insertion: print('# direct insertion!')\n",
    "\n",
    "nanals = 20 # ensemble members\n",
    "\n",
    "oberrstdev = 1. # ob error standard deviation in K\n",
    "oberrstdev_arctan = 0.01\n",
    "\n",
    "# nature run created using sqg_run.py.\n",
    "filename_climo = 'sqg_N64_12hrly.nc' # file name for forecast model climo\n",
    "# perfect model\n",
    "filename_truth = 'sqg_N64_12hrly.nc' # file name for nature run to draw obs\n",
    "\n",
    "print('# filename_modelclimo=%s' % filename_climo)\n",
    "print('# filename_truth=%s' % filename_truth)\n",
    "\n",
    "# fix random seed for reproducibility.\n",
    "rsobs = np.random.RandomState(42) # fixed seed for observations\n",
    "rsics = np.random.RandomState() # varying seed for initial conditions\n",
    "rsarctan = np.random.RandomState(98) # fixed seed for observations\n",
    "rsjump = np.random.RandomState(10) # fixed seed for observations\n",
    "\n",
    "# get model info\n",
    "nc_climo = Dataset(filename_climo)\n",
    "# parameter used to scale PV to temperature units.\n",
    "scalefact = nc_climo.f*nc_climo.theta0/nc_climo.g\n",
    "# initialize qg model instances for each ensemble member.\n",
    "x = nc_climo.variables['x'][:]\n",
    "y = nc_climo.variables['y'][:]\n",
    "x, y = np.meshgrid(x, y)\n",
    "nx = len(x); ny = len(y)\n",
    "dt = nc_climo.dt\n",
    "if diff_efold == None: diff_efold=nc_climo.diff_efold\n",
    "pvens = np.empty((nanals,2,ny,nx),np.float32)\n",
    "if not read_restart:\n",
    "    pv_climo = nc_climo.variables['pv']\n",
    "    indxran = rsics.choice(pv_climo.shape[0],size=nanals,replace=False)\n",
    "else:\n",
    "    ncinit = Dataset('%s_restart.nc' % exptname, mode='r', format='NETCDF4_CLASSIC')\n",
    "    ncinit.set_auto_mask(False)\n",
    "    pvens[:] = ncinit.variables['pv_b'][-1,...]/scalefact\n",
    "    tstart = ncinit.variables['t'][-1]\n",
    "    #for nanal in range(nanals):\n",
    "    #    print(nanal, pvens[nanal].min(), pvens[nanal].max())\n",
    "# get OMP_NUM_THREADS (threads to use) from environment.\n",
    "models = []\n",
    "for nanal in range(nanals):\n",
    "    if not read_restart:\n",
    "        pvens[nanal] = pv_climo[indxran[nanal]]\n",
    "        #print(nanal, pvens[nanal].min(), pvens[nanal].max())\n",
    "    pvens[nanal] = pv_climo[0] + np.random.normal(0,1000,size=(2,ny,nx))\n",
    "    models.append(\\\n",
    "    SQG(pvens[nanal],\n",
    "    nsq=nc_climo.nsq,f=nc_climo.f,dt=dt,U=nc_climo.U,H=nc_climo.H,\\\n",
    "    r=nc_climo.r,tdiab=nc_climo.tdiab,symmetric=nc_climo.symmetric,\\\n",
    "    diff_order=nc_climo.diff_order,diff_efold=diff_efold,threads=threads))\n",
    "if read_restart: ncinit.close()\n",
    "\n",
    "\n",
    "# vertical localization scale\n",
    "Lr = np.sqrt(models[0].nsq)*models[0].H/models[0].f\n",
    "vcovlocal_fact = gaspcohn(np.array(Lr/hcovlocal_scale))\n",
    "#vcovlocal_fact = 0.0 # no increment at opposite boundary\n",
    "#vcovlocal_fact = 1.0 # no vertical localization\n",
    "\n",
    "print('# use_letkf=%s global_enkf=%s' % (use_letkf,global_enkf))\n",
    "print(\"# hcovlocal=%g vcovlocal=%s diff_efold=%s covinf1=%s covinf2=%s nanals=%s\" %\\\n",
    "     (hcovlocal_scale/1000.,vcovlocal_fact,diff_efold,covinflate1,covinflate2,nanals))\n",
    "\n",
    "# if nobs > 0, each ob time nobs ob locations are randomly sampled (without\n",
    "# replacement) from the model grid\n",
    "# if nobs < 0, fixed network of every Nth grid point used (N = -nobs)\n",
    "nobs = -1024                                                         #2048  #nx*ny//16 # number of obs to assimilate (randomly distributed)\n",
    "#nobs = -1 # fixed network, every -nobs grid points. nobs=-1 obs at all pts.\n",
    "print('Obs Precentage is ', nobs/(nx*ny))\n",
    "# nature run\n",
    "nc_truth = Dataset(filename_truth)\n",
    "pv_truth = nc_truth.variables['pv']\n",
    "# set up arrays for obs and localization function\n",
    "if nobs < 0:\n",
    "    nskip = -nobs\n",
    "    # if (nx*ny)%nobs != 0:\n",
    "    #     raise ValueError('nx*ny must be divisible by nobs')\n",
    "    #nobs = (nx*ny)//nskip**2\n",
    "    fixed = True\n",
    "    nobs = nskip\n",
    "    print('# fixed network nobs = %s' % nobs)\n",
    "else:\n",
    "    fixed = False\n",
    "    print('# random network nobs = %s' % nobs)\n",
    "#if nobs == nx*ny//2: fixed=True # used fixed network for obs every other grid point\n",
    "print('fixed is', fixed)\n",
    "oberrvar = oberrstdev**2*np.ones(nobs,float)\n",
    "pvob = np.empty((2,nobs),float)\n",
    "covlocal = np.empty((ny,nx),float)\n",
    "covlocal_tmp = np.empty((nobs,nx*ny),float)\n",
    "xens = np.empty((nanals,2,nx*ny),float)\n",
    "if not use_letkf:\n",
    "    obcovlocal = np.empty((nobs,nobs),float)\n",
    "else:\n",
    "    obcovlocal = None\n",
    "\n",
    "if global_enkf: # model-space localization matrix\n",
    "    n = 0\n",
    "    covlocal_modelspace = np.empty((nx*ny,nx*ny),float)\n",
    "    x1 = x.reshape(nx*ny); y1 = y.reshape(nx*ny)\n",
    "    for n in range(nx*ny):\n",
    "        dist = cartdist(x1[n],y1[n],x1,y1,nc_climo.L,nc_climo.L)\n",
    "        covlocal_modelspace[n,:] = gaspcohn(dist/hcovlocal_scale)\n",
    "\n",
    "obtimes = nc_truth.variables['t'][:]\n",
    "if read_restart:\n",
    "    timeslist = obtimes.tolist()\n",
    "    ntstart = timeslist.index(tstart)\n",
    "    print('# restarting from %s.nc ntstart = %s' % (exptname,ntstart))\n",
    "else:\n",
    "    ntstart = 0\n",
    "assim_interval = obtimes[1]-obtimes[0]\n",
    "assim_timesteps = int(np.round(assim_interval/models[0].dt))\n",
    "print('# assim interval = %s secs (%s time steps)' % (assim_interval,assim_timesteps))\n",
    "print('# ntime,pverr_a,pvsprd_a,pverr_b,pvsprd_b,obinc_b,osprd_b,obinc_a,obsprd_a,omaomb/oberr,obbias_b,inflation,tr(P^a)/tr(P^b)')\n",
    "\n",
    "# initialize model clock\n",
    "for nanal in range(nanals):\n",
    "    models[nanal].t = obtimes[ntstart]\n",
    "    models[nanal].timesteps = assim_timesteps\n",
    "\n",
    "# initialize output file.\n",
    "if savedata is not None:\n",
    "   nc = Dataset('%s.nc' % exptname, mode='w', format='NETCDF4_CLASSIC')\n",
    "   nc.r = models[0].r\n",
    "   nc.f = models[0].f\n",
    "   nc.U = models[0].U\n",
    "   nc.L = models[0].L\n",
    "   nc.H = models[0].H\n",
    "   nc.nanals = nanals\n",
    "   nc.hcovlocal_scale = hcovlocal_scale\n",
    "   nc.vcovlocal_fact = vcovlocal_fact\n",
    "   nc.oberrstdev = oberrstdev\n",
    "   nc.g = nc_climo.g; nc.theta0 = nc_climo.theta0\n",
    "   nc.nsq = models[0].nsq\n",
    "   nc.tdiab = models[0].tdiab\n",
    "   nc.dt = models[0].dt\n",
    "   nc.diff_efold = models[0].diff_efold\n",
    "   nc.diff_order = models[0].diff_order\n",
    "   nc.filename_climo = filename_climo\n",
    "   nc.filename_truth = filename_truth\n",
    "   nc.symmetric = models[0].symmetric\n",
    "   xdim = nc.createDimension('x',models[0].N)\n",
    "   ydim = nc.createDimension('y',models[0].N)\n",
    "   z = nc.createDimension('z',2)\n",
    "   t = nc.createDimension('t',None)\n",
    "   obs = nc.createDimension('obs',nobs)\n",
    "   ens = nc.createDimension('ens',nanals)\n",
    "   pv_t =\\\n",
    "   nc.createVariable('pv_t',np.float32,('t','z','y','x'),zlib=True)\n",
    "   pv_c =\\\n",
    "   nc.createVariable('pv_c',np.float32,('t','ens','z','y','x'),zlib=True)\n",
    "   pv_b =\\\n",
    "   nc.createVariable('pv_b',np.float32,('t','ens','z','y','x'),zlib=True)\n",
    "   pv_a =\\\n",
    "   nc.createVariable('pv_a',np.float32,('t','ens','z','y','x'),zlib=True)\n",
    "   pv_a.units = 'K'\n",
    "   pv_b.units = 'K'\n",
    "   pv_c.units = 'K'\n",
    "   inf = nc.createVariable('inflation',np.float32,('t','z','y','x'),zlib=True)\n",
    "   pv_obs = nc.createVariable('obs',np.float32,('t','obs'))\n",
    "   x_obs = nc.createVariable('x_obs',np.float32,('t','obs'))\n",
    "   y_obs = nc.createVariable('y_obs',np.float32,('t','obs'))\n",
    "   # eady pv scaled by g/(f*theta0) so du/dz = d(pv)/dy\n",
    "   xvar = nc.createVariable('x',np.float32,('x',))\n",
    "   xvar.units = 'meters'\n",
    "   yvar = nc.createVariable('y',np.float32,('y',))\n",
    "   yvar.units = 'meters'\n",
    "   zvar = nc.createVariable('z',np.float32,('z',))\n",
    "   zvar.units = 'meters'\n",
    "   tvar = nc.createVariable('t',np.float32,('t',))\n",
    "   tvar.units = 'seconds'\n",
    "   ensvar = nc.createVariable('ens',np.int32,('ens',))\n",
    "   ensvar.units = 'dimensionless'\n",
    "   xvar[:] = np.arange(0,models[0].L,models[0].L/models[0].N)\n",
    "   yvar[:] = np.arange(0,models[0].L,models[0].L/models[0].N)\n",
    "   zvar[0] = 0; zvar[1] = models[0].H\n",
    "   ensvar[:] = np.arange(1,nanals+1)\n",
    "\n",
    "# initialize kinetic energy error/spread spectra\n",
    "kespec_errmean = None; kespec_sprdmean = None\n",
    "\n",
    "ncount = 0\n",
    "nanals2 = 4 # ensemble members used for kespec spread\n",
    "\n",
    "init_std_x_state = (pvens.reshape(nanals,2*nx*ny)).std(axis=0)\n",
    "\n",
    "#Jump\n",
    "jump = 3000\n",
    "jumpnoise = rsjump.normal(0,900,size=(1,2,ny,nx)) \n",
    "jumpnoise_reshape =jumpnoise.reshape(2,ny*nx) \n",
    "\n",
    "\n",
    "indxob = np.sort(rsobs.choice(nx*ny,nobs,replace=False))\n",
    "indx_unob = np.setdiff1d(np.arange(nx*ny), indxob)\n",
    "obs_save = np.zeros((100,nobs))\n",
    "\n",
    "for ntime in range(100): #nassim\n",
    "\n",
    "    # check model clock\n",
    "    # if models[0].t != obtimes[ntime+ntstart]:\n",
    "    #     raise ValueError('model/ob time mismatch %s vs %s' %\\\n",
    "    #     (models[0].t, obtimes[ntime+ntstart]))\n",
    "\n",
    "    t1 = time.time()\n",
    "    if not fixed:\n",
    "        if ntime == 0:\n",
    "            print('RRRRRRR')\n",
    "        # randomly choose points from model grid\n",
    "        if nobs == nx*ny:\n",
    "            indxob = np.arange(nx*ny)\n",
    "            indx_unob = np.setdiff1d(np.arange(nx*ny), indxob)\n",
    "        else:\n",
    "            indxob = np.sort(rsobs.choice(nx*ny,nobs,replace=False))\n",
    "            indx_unob = np.setdiff1d(np.arange(nx*ny), indxob)\n",
    "    else:\n",
    "        if ntime == 0:\n",
    "            print(\"not Random\")\n",
    "    obs_save[ntime] = indxob\n",
    "\n",
    "\n",
    "    indxob_ensf = np.concatenate((indxob, indxob+nx*ny), axis=None)\n",
    "    indxobs_rows = torch.tensor(indxob // (nx),device='cuda')\n",
    "    indxobs_cols = torch.tensor(indxob % (ny),device='cuda')\n",
    "    indx_unob_rows, indx_unob_cols = unused_indices(nx,indxobs_rows,indxobs_cols)\n",
    "    indx_unob_ensf = np.concatenate((indx_unob, indx_unob+nx*ny), axis=None)\n",
    "    \n",
    "    pickarctan =  nobs             #  0#                                                              ##############################################################\n",
    "    if ntime == 0:\n",
    "        print('ARCTAN  percetage is',pickarctan/nobs*100)\n",
    "    arctan_index =  np.sort(rsarctan.choice(nobs, pickarctan, replace=False)) #2048 1024 4096\n",
    "    arctan_index_ensf = np.concatenate((arctan_index, arctan_index+nobs), axis=None) \n",
    "    #print(ntime+ntstart)\n",
    "    for k in range(2):\n",
    "        # surface temp obs\n",
    "        if ntime >= jump and ntime <= jump+2:\n",
    "            pvob[k] = scalefact*(pv_truth[ntime+ntstart,k,:,:].ravel()[indxob] + jumpnoise_reshape[k,indxob])\n",
    "        else:\n",
    "            pvob[k] = scalefact*pv_truth[ntime+ntstart,k,:,:].ravel()[indxob]\n",
    "        pvob[k,arctan_index] = np.arctan(pvob[k,arctan_index]) + rsobs.normal(scale=oberrstdev_arctan,size=int(pickarctan)) # add ob errors int(nobs/2)\n",
    "        pvob[k,~np.isin(np.arange(len(pvob[k])), arctan_index)] += rsobs.normal(scale=oberrstdev,size=int(nobs - pickarctan)) # add ob errors\n",
    "    \n",
    "  \n",
    "    pvensmean_b = pvens.mean(axis=0).copy()\n",
    "\n",
    "    if ntime >= jump and ntime <= jump+2:\n",
    "        pverr_b = (scalefact*(pvensmean_b-pv_truth[ntime+ntstart]+jumpnoise))**2\n",
    "    else:\n",
    "        pverr_b = (scalefact*(pvensmean_b-pv_truth[ntime+ntstart]))**2\n",
    "    pvsprd_b = ((scalefact*(pvensmean_b-pvens))**2).sum(axis=0)/(nanals-1)\n",
    "\n",
    "    if savedata is not None:\n",
    "        if savedata == 'restart' and ntime != nassim-1:\n",
    "            pass\n",
    "        else:\n",
    "            pv_t[ntime] = pv_truth[ntime+ntstart]\n",
    "            pv_b[ntime,:,:,:] = scalefact*pvens\n",
    "            #pv_obs[ntime] = pvob\n",
    "            #x_obs[ntime] = xob\n",
    "            #y_obs[ntime] = yob\n",
    "\n",
    "    # EnKF update\n",
    "    EnSF_Update_obs = EnSF(n_dim = nobs*2, ensemble_size = nanals ,eps_alpha=0.05, device= 'cuda' ,\\\n",
    "                   obs_sigma = oberrstdev, euler_steps = 1000, scalefact = nc_climo.f*nc_climo.theta0/nc_climo.g, init_std_x_state = init_std_x_state[indxob_ensf],  ISarctan=False)\n",
    "    EnSF_Update_unobs = EnSF(n_dim = nx*ny*2 - nobs*2, ensemble_size = nanals ,eps_alpha=0.05, device= 'cuda' ,\\\n",
    "                obs_sigma = oberrstdev, euler_steps = 1000, scalefact = nc_climo.f*nc_climo.theta0/nc_climo.g, init_std_x_state = init_std_x_state[indx_unob_ensf],  ISarctan=False)\n",
    "\n",
    "\n",
    "    # create 1d state vector.\n",
    "    xens = pvens.reshape(nanals,2*nx*ny).copy()\n",
    "    # update state vector.\n",
    "    xens_obs =\\\n",
    "    EnSF_Update_obs.state_update_normalized(x_input = xens[:,indxob_ensf],obs_input = pvob.reshape(2*nobs), arcindex=arctan_index_ensf)\n",
    "    xens[:,indxob_ensf] = xens_obs.cpu().numpy()\n",
    "    xens = xens.reshape(nanals,2,nx,ny)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    img_mask = np.zeros((ny, nx))\n",
    "    img_mask[indxobs_rows.cpu().numpy(),indxobs_cols.cpu().numpy()] = 1.0\n",
    "    masked_img = xens.reshape(40,nx,ny)* scalefact\n",
    "    masked_img_min = np.min(masked_img,axis = (1,2))[:, None, None] #\n",
    "    masked_img_max = np.max(masked_img,axis = (1,2))[:, None, None]  #\n",
    "    masked_img = (masked_img - masked_img_min)/ (masked_img_max - masked_img_min)\n",
    "    masked_img = masked_img * img_mask\n",
    "    recovered_img = np.zeros((40,nx,ny))\n",
    "    #cv2\n",
    "    cv2_raw_img = (masked_img * 255.0).astype(np.uint8)\n",
    "    cv2_mask = ((1-img_mask) * 255.0).astype(np.uint8)\n",
    "\n",
    "    results_img = Parallel(n_jobs=-1)(delayed(process_image)(i) for i in range(40))\n",
    "    recovered_img[:] = np.array(results_img)\n",
    "\n",
    "    recovered_img = recovered_img * (masked_img_max - masked_img_min) + masked_img_min\n",
    "\n",
    "    xens = xens.reshape(nanals,2*nx*ny)\n",
    "    unobs = recovered_img.reshape(nanals,2*nx*ny)[:,indx_unob_ensf] #* scalefact\n",
    "    xens_unobs =\\\n",
    "    EnSF_Update_unobs.un_state_update_normalized(x_input = xens[:,indx_unob_ensf],obs_input = unobs, arcindex=np.array([]))\n",
    "    xens[:,indx_unob_ensf] = xens_unobs.cpu().numpy()\n",
    "    # back to 3d state vector\n",
    "    pvens = xens.reshape((nanals,2,ny,nx)).copy()\n",
    "\n",
    "    t2 = time.time()\n",
    "    #print('cpu time for EnKF update',t2-t1)\n",
    "\n",
    "    if savedata is not None:\n",
    "        if savedata == 'restart' and ntime != nassim-1:\n",
    "            pass\n",
    "        else:\n",
    "            pv_c[ntime,:,:,:] = scalefact*pvens\n",
    "    pvensmean_a = pvens.mean(axis=0)\n",
    "    # print out analysis error, spread and innov stats for background\n",
    "    if ntime >= jump and ntime <= jump+2:\n",
    "        print('jumpppp')\n",
    "        pverr_a = (scalefact*(pvensmean_a-pv_truth[ntime+ntstart]-jumpnoise))**2\n",
    "        print(ntstart)\n",
    "    else:\n",
    "        pverr_a = (scalefact*(pvensmean_a-pv_truth[ntime+ntstart]))**2\n",
    "    pvsprd_a = ((scalefact*(pvensmean_a-pvens))**2).sum(axis=0)/(nanals-1)\n",
    "    print(\"%s %g %g %g %g \" %\\\n",
    "    (ntime+ntstart,np.sqrt(pverr_a.mean()),np.sqrt(pvsprd_a.mean()),np.sqrt(pverr_b.mean()),np.sqrt(pvsprd_b.mean())))\n",
    "\n",
    "    # save data.\n",
    "    if savedata is not None:\n",
    "        if savedata == 'restart' and ntime != nassim-1:\n",
    "            pass\n",
    "        else:\n",
    "            pv_a[ntime,:,:,:] = scalefact*pvens\n",
    "            tvar[ntime] = obtimes[ntime+ntstart]\n",
    "            #inf[ntime] = inflation_factor\n",
    "            nc.sync()\n",
    "\n",
    "    # run forecast ensemble to next analysis time\n",
    "    t1 = time.time()\n",
    "    # for nanal in range(nanals):\n",
    "    #     pvens[nanal] = models[nanal].advance(pvens[nanal])\n",
    "    pvens_updated = Parallel(n_jobs=-1)(delayed(advance_task)(nanal) for nanal in range(nanals))\n",
    "    for nanal, updated_pven in enumerate(pvens_updated):\n",
    "        pvens[nanal] = updated_pven\n",
    "    t2 = time.time()\n",
    "    if ntime < 10:\n",
    "        print('cpu time for ens forecast',t2-t1)\n",
    "   \n",
    "if savedata: nc.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
